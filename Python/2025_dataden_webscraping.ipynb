{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Web Scraping with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is a way to collect information from websites using code. It can be especially useful when working with data that isn’t easily downloadable. There are several approaches and tools for web scraping—this workshop will focus on one of them: Python’s Beautiful Soup package. Python is an open-source language, so with the right setup, anyone can use this tool.\n",
    "\n",
    "Web scraping can support different stages of the research data lifecycle, including the planning phase (e.g., identifying available online data) and the active data collection phase. This workshop is intended for those who are new to web scraping and want to explore how it can be used in a research context.\n",
    "\n",
    "The session is hosted by Cornell University Library’s Research Data & Open Scholarship team and is part of the Data Den workshop series. \n",
    "\n",
    "\n",
    "Sharing statement: CC-By Attribution 4.0 International"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"contents\">Contents</a>\n",
    "* <a href='#intro'>Introduction</a>\n",
    "    - <a href='#summary'>Workshop Summary</a> \n",
    "    - <a href='#presenters'>Presenters</a>\n",
    "    - <a href='#collaborators'>Collaborators</a>\n",
    "    - <a href='#objectives'>Learning Objectives</a> \n",
    "    - <a href='#knowledge'>Assumed Knowledge</a>\n",
    "    - <a href='#logistics'>Logistics</a>\n",
    "* <a href = \"#problems\">Exercises</a>\n",
    "* <a href='#resources'>Additional Resources</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id = \"intro\"></a>Introduction\n",
    "\n",
    "## <a id = \"summary\"></a>Workshop Summary\n",
    "Learn how to gather data from websites using Python! In this beginner-friendly workshop, you’ll learn the basics of web scraping with Beautiful Soup. We’ll show you how to dig through HTML to find the info you need, and talk about when it makes more sense to use an API or tools like Selenium. You’ll also get tips on cleaning up your data so it’s ready to use. \n",
    "\n",
    "\n",
    "## <a id = \"presenters\"></a>Instructor\n",
    "\n",
    "Jacob Grippin\\\n",
    "Statistical Consultant\\\n",
    "Cornell Center for Social Sciences\\\n",
    "jrg363@cornell.edu \n",
    "\n",
    "\n",
    "## <a id = \"collaborators\"></a>Collaborators\n",
    "Iliana Burgos\\\n",
    "Emerging Data Practices Librarian\\\n",
    "Digital Scholarship Services\\\n",
    "itb23@cornell.edu \n",
    "\n",
    "Lencia McKee\\\n",
    "Research Data Librarian\\\n",
    "Research Data & Open Scholarship\\\n",
    "lcb235@cornell.edu \n",
    "\n",
    "## <a id = \"objectives\"></a>Learning Objectives\n",
    "\n",
    "Workshop attendees will:\n",
    "1. **Basics of beautiful soup**: Learners will be able to use the beautiful soup Python package to parse through HTML (such as tags and attributes) and extract content from webpages relevant to their research questions. \n",
    "2. **Comparing methods and packages**: Learners will be able to compare different approaches (web scraping vs. APIs) and tools (Beautiful Soup vs. Selenium) and select the most appropriate one on their skill levels and needs. \n",
    "3. **Data preparation**: Learners will understand how to clean, structure, and export scraped data to make it ready for analysis. \n",
    "\n",
    "## <a id = \"knowledge\"></a>Assumed Knowledge\n",
    "\n",
    "This workshop is for folks who are new to Python but have at least a little coding experience. You don’t need to be an expert, but some familiarity with basic programming ideas will be helpful. We won’t be covering the very basics of Python, so it's best if you've seen a bit of code before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and/or load libraries that we will use in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21037,
     "status": "ok",
     "timestamp": 1747853490451,
     "user": {
      "displayName": "Jacob Grippin",
      "userId": "02814088970123264197"
     },
     "user_tz": 240
    },
    "id": "sTxtYTDILehH",
    "outputId": "0ebc1985-85bd-4145-9d2a-09b2a51ab723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\jrg363\\appdata\\roaming\\python\\python312\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in c:\\users\\jrg363\\appdata\\roaming\\python\\python312\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install pandas\n",
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import re\n",
    "import os\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id = \"problems\"></a>Exercises\n",
    "\n",
    "## <a id = \"beautifulsoup\"></a>BeautifulSoup\n",
    "\n",
    "<p id = \"exercise_desc\">In this exercise we will look at the University Postings from Cornell President Martha Pollack. You can view the postings <a href = \"https://statements.cornell.edu/pollack.cfm\">here</a>. The objective is to take each articles title, content, date, and URL. Then save the the results into a data frame containing article title, date and URL. Along with a word document containing the full artcile  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I4PUqT5dL4ri"
   },
   "outputs": [],
   "source": [
    "#Open the page containing University Statements by President Pollack\n",
    "html = urlopen('https://statements.cornell.edu/pollack.cfm')\n",
    "#Use the BeautifulSoup package to load the content. \n",
    "bs_html = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Tags & Attributes\n",
    "\n",
    "- **Tags** are used to represent different component types within an HTML file, such as the `<title>`, `<div>` for a designated section within the document, `<h1>` for the first header or `<a>` for links.\n",
    "- HTML tags usually require a closing tag such as </title> or </div>. Sections in HTML begin and end.\n",
    "- HTML tags will often include additional information within the tag itself that are known as **attributes**. You can think of attributes as an additional identifier for a given tag.\n",
    "- Attributes are marked by key words such as `class`, `id`, or `href` (referring to hyperlinks) followed by an equal sign and a label such as `<div class='container'>`\n",
    "\n",
    "Here are two external references for learning more about the available [tags](https://www.w3schools.com/tags/default.asp) and [attributes](https://www.w3schools.com/tags/ref_attributes.asp) within HTML.\n",
    "\n",
    "\n",
    "Our created Beautiful Soup parsed HTML object offers a variety of methods that allows us to look at specific tags within the HTML. The following retrieves all specified instances of the `a` tag containing links in the HTML file:\n",
    "\n",
    "Let's spend some time talking about the Inspect feature. Right click on the item you want to extract, and click on 'inspect'. More information about the <a href = \"https://www.theodinproject.com/lessons/foundations-inspecting-html-and-css\">inspector feature</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "P1dSicL-L6oA"
   },
   "outputs": [],
   "source": [
    "#Use the find_all feature to find all article links. \n",
    "links = bs_html.find_all('a', {'class':'cu-statement-title'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes are used to distinguish different subgroups of the same base HTML tag. This facilitates the easy retrieval of distinct HTML elements within a web scraping program that we would otherwise struggle to differentiate due to having the same tags. An example of this is the `<a class=\"cu-statement-title\">`to distinguish between the statement URLs from other URLS found on the page.\n",
    "\n",
    "Let's look at the results. Just checking out the first result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"cu-statement-title\" href=\"../2024/20240531-recommendations.cfm\">External advisory committee recommendations</a>\n"
     ]
    }
   ],
   "source": [
    "print(links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping to get URL\n",
    "The above code extract all the HTML associated with the `<a>` tags that have the class attribute as 'cu-statement-title'. But when we print, it still looks messy. Let's incorporate a loop to go through all the results, and only extract the `href` attribute that provides us with an individual article URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "THZ6lGrbL_KH"
   },
   "outputs": [],
   "source": [
    "link_url = []\n",
    "for i in links:\n",
    "    link_url.append(i['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the contents of the list. The first line of code below will display the number of URLs we have extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result shows there are 111 articles in this page. Let's look at the first 10 using the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../2024/20240531-recommendations.cfm', '../2024/20240530-student-referendum.cfm', '../2024/20240514-encampment-update.cfm', 'https://statements.cornell.edu/2024/20240509-some-news.cfm', 'https://statements.cornell.edu/2024/20240429-campus-events.cfm', 'https://statements.cornell.edu/2024/20240422-patient-safety-wcm.cfm', '../2024/20240419-resources.cfm', '../2024/20240327-community.cfm', '../2024/20240319-incident-in-collegetown.cfm']\n"
     ]
    }
   ],
   "source": [
    "print(link_url[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up\n",
    "Notice from the results above. Some of the URLs we extracted contain a full URL. That's good. However, some do not. They only contain paths relative to the current page. This will not do. We must find a solution so all the URLs in our list are full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hJ9FYcpKMA0R"
   },
   "outputs": [],
   "source": [
    "base = \"https://statements.cornell.edu/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VnFcFILsMD9T"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in link_url:\n",
    "    if \"http\" not in i:\n",
    "        link_url[count] = base + i\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dvyyS19qMGVA"
   },
   "outputs": [],
   "source": [
    "link_url = list(map(lambda st: st.replace('../', ''), link_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the above process\n",
    "Looking at the URLs manually from the articles page, I can see each URL has the base of \"https://statements.cornell.edu/\". So the URLs from my previous printing procedure that are not full, they just need the base of \"https://statements.cornell.edu/\" added to the beginning. And they would also need the '../' removed from the URLs to make them all full. Using an if condition, we check to see if 'http' is present in each of our links. If it is not present, the base will be added. \n",
    "\n",
    "Let's look at the first 10 results now. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://statements.cornell.edu/2024/20240531-recommendations.cfm', 'https://statements.cornell.edu/2024/20240530-student-referendum.cfm', 'https://statements.cornell.edu/2024/20240514-encampment-update.cfm', 'https://statements.cornell.edu/2024/20240509-some-news.cfm', 'https://statements.cornell.edu/2024/20240429-campus-events.cfm', 'https://statements.cornell.edu/2024/20240422-patient-safety-wcm.cfm', 'https://statements.cornell.edu/2024/20240419-resources.cfm', 'https://statements.cornell.edu/2024/20240327-community.cfm', 'https://statements.cornell.edu/2024/20240319-incident-in-collegetown.cfm']\n"
     ]
    }
   ],
   "source": [
    "print(link_url[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Date, Title and content of each Article. Looping Again\n",
    "Now each URL is full. Excellent! We are ready to move to the next step. We will bring in another loop. The plan is to go to each article page, reach the HTML code from it. And get the information we want (title, date, article content). Then save each article as an individual word file.\n",
    "\n",
    "Lets talk about the inspect feature again. Right click on the item you want to extract, and click on 'inspect'. More information about the <a href = \"https://www.theodinproject.com/lessons/foundations-inspecting-html-and-css\">inspector feature</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VG7VONlsMIbU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jrg363\\\\Workshops FA25\\\\Web Scraping Library'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display where word files will get saved\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143183,
     "status": "ok",
     "timestamp": 1747853786903,
     "user": {
      "displayName": "Jacob Grippin",
      "userId": "02814088970123264197"
     },
     "user_tz": 240
    },
    "id": "pr01YyNGMcnf",
    "outputId": "7d2fe885-aaf9-4671-b2a3-8759d6e91011"
   },
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "#Loop through each individual URL\n",
    "for i in link_url:\n",
    "    #Open URL\n",
    "    html = urlopen(i)\n",
    "    #Store html code using beautifulSoup\n",
    "    bs_html = BeautifulSoup(html, 'html.parser')\n",
    "    #Find the title and get the associated text.\n",
    "    title = bs_html.find('h2', {'class':'cu-headline'})\n",
    "    title = title.get_text()\n",
    "    #Find the date and get the associated text.\n",
    "    date = bs_html.find('time', {'class':'news-date'})\n",
    "    date = date.get_text()\n",
    "    #Clean up the date a bit. \n",
    "    date = re.sub(r'\\s+', ' ',date).strip()\n",
    "    #Find the article content and get the associated text.\n",
    "    paragraphs = bs_html.find_all('p')\n",
    "    #Create a name for the word document that will be saved. \n",
    "    name = title + \"_\" + date\n",
    "    name = re.sub(r'[^A-Za-z0-9 ]+', '_', name)\n",
    "    #Save content to word file. \n",
    "    filename = current_directory + \"\\\\\" + name + \".doc\" \n",
    "    #print(filename)\n",
    "    doc = Document()\n",
    "    doc.add_heading(title)\n",
    "    for p in paragraphs:\n",
    "        text = p.get_text()\n",
    "        doc.add_paragraph(text)\n",
    "    #Save word document. \n",
    "    doc.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing Up\n",
    "We already have a word file for each article. We could use that for text or sentiment analysis. Let's also create a dataframe that consists of article title, article date, and article URL. This information we already know how to get from the above section of code. The code below combines that into a python pandas dataframe and exports to an excel spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "titles = []\n",
    "for i in link_url:\n",
    "    #Open URL\n",
    "    html = urlopen(i)\n",
    "    #Store html code using beautifulSoup\n",
    "    bs_html = BeautifulSoup(html, 'html.parser')\n",
    "    #Find the title and get the associated text.\n",
    "    title = bs_html.find('h2', {'class':'cu-headline'})\n",
    "    title = title.get_text()\n",
    "    #Append title to a list\n",
    "    titles.append(title)\n",
    "    #Find the date and get the associated text.\n",
    "    date = bs_html.find('time', {'class':'news-date'})\n",
    "    date = date.get_text()\n",
    "    #Clean up the date a bit. \n",
    "    date = re.sub(r'\\s+', ' ',date).strip()\n",
    "    #Append date to list\n",
    "    dates.append(date)\n",
    "\n",
    "#Combine the results into a dataframe. \n",
    "article_data = pd.DataFrame({\n",
    "    'title': titles,\n",
    "    'date': dates,\n",
    "    'url': link_url,\n",
    "})\n",
    "current_directory = os.getcwd()\n",
    "filename = current_directory + \"\\\\\" + \"article_data\" + \".xlsx\"\n",
    "#Export to Excel\n",
    "article_data.to_excel(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id = \"resources\"></a>Additional Resources\n",
    "\n",
    "- <a></a>[Beautiful Soup Documentation](https://beautiful-soup-4.readthedocs.io/en/latest/#making-the-soup)\n",
    "\n",
    "- <a href = \"https://www.geeksforgeeks.org/html/tags-vs-elements-vs-attributes-in-html/\"> HTML tags, elements and attributes</a>\n",
    "\n",
    "- <a href = \"https://www.tutorialspoint.com/html/html_attributes.htm\">What are HTML Attributes</a>\n",
    "\n",
    "- <a href= \"https://www.w3schools.com/tags/ref_attributes.asp\">List of all HTML Attributes</a>\n",
    "\n",
    "- <a href = \"https://www.w3schools.com/TAGS/default.asp\">List of all HTML Tags</a>\n",
    "\n",
    "- <a href = \"https://jsonapi.org/examples/\">Sample API Json data</a>\n",
    "\n",
    "- <a href = \"https://socialsciences.cornell.edu/computing-and-data/workshops-and-training\">List of Fall 2025 CCSS Workshops</a>\n",
    "\n",
    "- <a href = \"https://socialsciences.cornell.edu/computing-and-data/schedule-a-consultation\">Schedule a 1v1 consultation with CCSS Staff</a>\n",
    "\n",
    "- <a href = \"https://www.geeksforgeeks.org/python/python-basics/\">Python Basics</a>\n",
    "\n",
    "- <a href = \"https://colab.research.google.com/drive/1FM2lQlVqkq8t1gu9paKacfcnLIfAHZKV?authuser=1&usp=drive_link#scrollTo=q8L6UD2DawPL\">Sample BeautifulSoup Python Script File and Guide created by CCSS</a>\n",
    "\n",
    "- <a href = \"https://vod.video.cornell.edu/media/Web+Scraping+in+Python%28BeautifulSoup%29/1_7v2s9fgz/319524772\">CCSS Python BeautifulSoup Workshop Recording Fall 2023</a>\n",
    "\n",
    "- <a href = \"https://www.geeksforgeeks.org/web-scraping/scrape-table-from-website-using-python-selenium/\">Python Selenium</a>\n",
    "\n",
    "- <a href = \"https://colab.research.google.com/drive/1u5kBOxzMH3ER4kRLuOJlKXfOVtofbE7J?usp=sharing#scrollTo=7H5_OqDn6tbm\">Sample Selenium Python Script File and Guide created by CCSS</a>\n",
    "\n",
    "- <a href = \"https://vod.video.cornell.edu/media/Intermediate+Web+Scraping+in+Python+%28Selenium%29/1_jrv69o2p/319524772\">CCSS Python Selenium Workshop Recording Fall 2023</a>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1C4UJKgosenDSlzRj7obbEtywitjbHD4M",
     "timestamp": 1748524160166
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
